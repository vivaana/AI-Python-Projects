# -*- coding: utf-8 -*-
"""Chatbot

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1KpSWAhhQ7shwzt1hrSx9VGxCtHaS7g6G
"""

!pip install google-cloud-vision

!pip install tensorflow==2.12.0

def read_dictionary(file_path):
  responses = {}

  try:
    file = open(file_path, "r")
    lines = file.readlines()
    file.close()
    for line in lines:
      key, value = line.split(":")
      responses[key] = value
    return responses
  except:
    print("Error: Make sure your file is called", file_path)


class Agent:
    def can_handle(self,  text):
        pass

    def get_response(self, text):
        pass


class ConditionalAgent(Agent):
    def __init__(self, file_path):
      self.conditional_responses = read_dictionary(file_path)
#Landmark Mode function------------------------------------------------------------------------------------------------------
    def landmark():
      import io
      from google.cloud import vision

      client = vision.ImageAnnotatorClient.from_service_account_json('creds.json')
      landy_pic = input("ðŸ¤– Please enter the filepath for your image: ")
      try:
        print(f"{landy_pic} was accepted!")

        with io.open(landy_pic, "rb") as image_file:
          content = image_file.read()

        image = vision.Image(content=content)
        response = client.landmark_detection(image = image)
        landmarks = response.landmark_annotations

        print("âŽ¯âŽ¯âŽ¯âŽ¯âŽ¯âŽ¯âŽ¯âŽ¯âŽ¯âŽ¯âŽ¯âŽ¯âŽ¯âŽ¯âŽ¯âŽ¯âŽ¯âŽ¯")

        if len(landmarks) == 0:
          print("ðŸ¤– No landmarks found")
          print("âŽ¯âŽ¯âŽ¯âŽ¯âŽ¯âŽ¯âŽ¯âŽ¯âŽ¯âŽ¯âŽ¯âŽ¯âŽ¯âŽ¯âŽ¯âŽ¯âŽ¯âŽ¯")
          print()

        else:
          for landmark in landmarks:
            print(f"ðŸ¤– {landmark.description}")
            print("âŽ¯âŽ¯âŽ¯âŽ¯âŽ¯âŽ¯âŽ¯âŽ¯âŽ¯âŽ¯âŽ¯âŽ¯âŽ¯âŽ¯âŽ¯âŽ¯âŽ¯âŽ¯")
            print()

      except:
        print("Error: Make sure your filepath is valid.")
        ConditionalAgent.landmark()

        print("\n------------------------------------------------------------------------------------------------------------------------------------------------------------------------")
#Number Mode function------------------------------------------------------------------------------------------------------
    def number_recognition():
      from keras.models import load_model  # TensorFlow is required for Keras to work
      from PIL import Image, ImageOps  # Install pillow instead of PIL
      import numpy as np

      model = load_model("my_model_nummy.h5")
      image_prediction = input("ðŸ¤– Please enter the filepath for your image: ")

      try:
        def image_predictor(image_name):
          img = np.invert(Image.open(image_name).convert("L"))
          img = img.reshape(1, 28, 28, 1)
          img = img.astype("float32")
          img /= 255

          preddy = model.predict(img)
          print(np.argmax(preddy))

        image_predictor(image_prediction)

      except:
        print("Please enter a valid filepath: ")
        ConditionalAgent.number_recognition()
      print(f"")
      print("\n------------------------------------------------------------------------------------------------------------------------------------------------------------------------")
#Emotion Mode function------------------------------------------------------------------------------------------------------
    def emotion_recognition():
      import io
      from google.cloud import vision

      client = vision.ImageAnnotatorClient.from_service_account_json('creds.json')

      image_to_see = input("Please enter the filepath to analyse: ")

      with io.open(image_to_see, "rb") as image_file:
        content = image_file.read()

      image = vision.Image(content=content)

      response = client.face_detection(image = image)
      faces = response.face_annotations

      print(f"ðŸ¤– There are {len(faces)} people in this picture!")
      print("---------------------------------------------")

      likelihood_name = ("unknown", "very unlikely", "unlikely", "possibly", "likely", "very likely")
      count = 0

      for face in faces:
        #print(f'\tPosition {face.bounding_poly}')
        count += 1
        print(f"Person {count}")
        print(f"\tThis person is {likelihood_name[face.joy_likelihood]} to be happy")
        print(f"\tThis person is {likelihood_name[face.sorrow_likelihood]} to be sad")
        print(f"\tThis person is {likelihood_name[face.anger_likelihood]} to be angry")
        print(f"\tThis person is {likelihood_name[face.surprise_likelihood]} to be surprised")
      print(f"")
      print("\n------------------------------------------------------------------------------------------------------------------------------------------------------------------------")
#Research Mode function------------------------------------------------------------------------------------------------------
    def Research_Mode():
      from google.cloud import language
      client = language.LanguageServiceClient.from_service_account_json("creds.json")
      print("Enter a question, or sentence and I will do a wikipedia Search!")
      print("To get a wikipedia link on a specific word, try and capitalise it.")
      text = input("Enter your sentence/question: ")

      document = language.Document(content=text, type=language.Document.Type.PLAIN_TEXT)
      response = client.analyze_entities(document=document, encoding_type= "UTF32")
      entities = response.entities

      for entity in entities:
        print(entity.name)
        print(language.Entity.Type(entity.type).name)
        print(entity.metadata["wikipedia_url"])

    def Image_analyser():
      import io
      from google.cloud import vision

      client = vision.ImageAnnotatorClient.from_service_account_json('creds.json')
      try:
        image_to_check = input("Please enter the filepath for your image so I can guess it.")

        with io.open(image_to_check, "rb") as image_file:
          content = image_file.read()

        image = vision.Image(content=content)

        response = client.label_detection(image = image)
        labels = response.label_annotations

        for label in labels:
          print(label.description, label.score)
      except:
        print("Sorry, please enter a valid filepath.")
        ConditionalAgent.Image_analyser()
#Animal Mode function------------------------------------------------------------------------------------------------------
    def Animals():

      animal_image = input("Please input the filepath below to see whether it is a cat, dog, monkey or a squirrel: ")

      try:
        from keras.models import load_model  # TensorFlow is required for Keras to work
        from PIL import Image, ImageOps  # Install pillow instead of PIL
        import numpy as np

        np.set_printoptions(suppress=True)

        model = load_model("latest_teachable_model.h5", compile=False)

        class_names = open("labels.txt", "r").readlines()

        data = np.ndarray(shape=(1, 224, 224, 3), dtype=np.float32)

        image = Image.open(animal_image).convert("RGB")

        size = (224, 224)
        image = ImageOps.fit(image, size, Image.Resampling.LANCZOS)

        image_array = np.asarray(image)

        normalized_image_array = (image_array.astype(np.float32) / 127.5) - 1

        data[0] = normalized_image_array

        prediction = model.predict(data)
        index = np.argmax(prediction)
        class_name = class_names[index]
        confidence_score = prediction[0][index]

        print("Class:", class_name[2:], end="")
        print("Confidence Score:", confidence_score)

        pred = model.predict(data)
        print(pred)
        print(class_names)

      except:
        print("Sorry, please enter a valid filepath: ")
        ConditionalAgent.Animals()
#Can_handle function------------------------------------------------------------------------------------------------------
    def can_handle(self, text):
      not_sure = []
      if text in self.conditional_responses:
        #print(f"DEBUG: Response for {text} is {self.conditional_responses[text]}")
        result = self.conditional_responses[text] is not None
        if result:
          print(f"")
        else:
          print("_____________________________________________")
          #print(f"ConditionalAgent cannot handle text {text}")
        return result

        return self.conditional_responses[text]
      else:
        #print(f"ConditionalAgent cannot handle text '{text}'")
        not_sure.append(text)
#Get_Response function------------------------------------------------------------------------------------------------------
    def get_response(self, text):
      if text in self.conditional_responses:
        #print(f"DEBUG: Response for {text} is {self.conditional_responses[text]}")
        return self.conditional_responses[text]
      elif text == "landmark":
        ConditionalAgent.landmark()
      elif text == "number":
        ConditionalAgent.number_recognition()
      elif text == "research":
          ConditionalAgent.Research_Mode()
      elif text == "emotion":
          ConditionalAgent.emotion_recognition()
      elif text == "animal":
          ConditionalAgent.Animal()

      else:
        print("_____________________________________________")
        print(f"ConditionalAgent cannot handle text '{text}'")

class FunTechBot:
    def __init__(self):
        self.agents = []

    def add_agent(self, agent):
        self.agents.append(agent)

    def get_response(self, text):
        for agent in self.agents:
            if agent.can_handle(text):
                return agent.get_response(text)

        return "I'm sorry I don't understand " + text + " \n"

print("Welcome to chat GuptaV!")
print("If you want to know the list of commands, then enter commands.")
print("For the best experience, please upload all necessary files, like responses.txt and creds.json, please.")
print()
chatbot = FunTechBot()
simple_agent = ConditionalAgent("responses.txt")
chatbot.add_agent(simple_agent)

while True:
  loser_input = input("ðŸ™‚ Input: ")
  user_input = loser_input.lower()
  if user_input == "landmark":
    ConditionalAgent.landmark()
  if user_input == "commands":
    print("Talk normally:\t\t\t type anything")
    print("Landmark detection:\t\t 'landmark'")
    print("Entity recognition & research:\t 'research'")
    print("Emotion detection:\t\t 'emotion'")
    print("Animal detection:\t\t 'animal'")
    print("Image detection:\t\t 'image'")
    print("Quit/exit conversation:\t\t 'goodbye'\n")
  #if user_input == "number":
    #ConditionalAgent.number_recognition()
  if user_input == "research":
    ConditionalAgent.Research_Mode()
  if user_input == "emotion":
    ConditionalAgent.emotion_recognition()
  if user_input == "animal":
    ConditionalAgent.Animals()
  if user_input == "image":
    ConditionalAgent.Image_analyser()
  if user_input == "goodbye":
    print("ðŸ¤–. Goodbye! Have a great day!")
    break
  response = chatbot.get_response(user_input)
  print("ðŸ¤– Output:", response)